---
title: hive整合hadoop通过flume+hue收集日志
date: 2016-06-30
tags:
- hive
- hadoop
- flume
- hue
categories:
 - System
---




hive作为hadoop集群架构之上的一个架构，通过一种类SQL的解析引擎来将作业转换成map/reduce执行的任务。

本文主要是分享基本的安装与使用经验。hive的实质是将表对应到HDFS中的目录。


## 添加环境变量

```bash
export JAVA_HOME="/usr/local/jdk1.7.0_09"
export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib
export PATH=$PATH:$JAVA_HOME/bin

export HADOOP_HOME="/home/hadoop/hadoop-2.6.4"
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/
export CLASSPATH=.:$CLASSPATH:$HADOOP_HOME/lib
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_USER_CLASSPATH_FIRST=true

export HIVE_HOME="/home/hadoop/apache-hive-2.1.0-bin"
export CLASSPATH=$CLASSPATH:$HIVE_HOME/lib
export PATH=$PATH:$HIVE_HOME/bin:$HIVE_HOME/conf
```

## 设置公钥本机免登陆密码

```bash
ssh-keygen
cp /root/.ssh/id_rsa.pub  /root/.ssh/authorized_keys
```




## 安装java环境

```bash
tar zxvf jdk-7u9-linux-x64.tar.gz
mv jdk1.7.0_09/ /usr/local/

ln -s /usr/local/jdk1.7.0_09/bin/java /usr/bin/
ln -s /usr/local/jdk1.7.0_09/bin/javac /usr/bin/
ln -s /usr/local/jdk1.7.0_09/bin/jar /usr/bin/
```


## 安装hadoop

```bash
wget http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz
tar zxvf hadoop-2.6.4.tar.gz
cd hadoop-2.6.4
vim etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/local/jdk1.7.0_09
vim etc/hadoop/core-site.xml

    <property>
        <name>hadoop.tmp.dir</name>

        <value>/home/ebuy/hadoop/hadoop-2.6.4/tmp</value>
		<!-- 如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop
		而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。 -->

        <description>A base for other temporary directories.</description>

    </property>

<!-- file system properties -->

    <property>

        <name>fs.default.name</name>

        <value>hdfs://localhost:9000</value>

    </property>
```

## 启动及验证hadoop

### 格式化HDFS文件系统

``hadoop namenode -format``

## 启动hadoop

``sh sbin/start-all.sh``

http://10.127.0.5:50070

## hdfs常用命令

```bash
hdfs dfs -mkdir /tmp/hive
hdfs dfs -ls  /tmp/hive/*
hdfs dfs -cat /tmp/hive/123 |  wc -l
hdfs dfs -chmod 755 /tmp/
hdfs dfs -rmr /tmp/*
hdfs dfs -rm -r -f -skipTrash  /ecoupon/cachemanage
hdfs dfs -mkdir /user/hive/warehouse
hdfs dfs -chmod 755 /user/hive/warehouse
```


## 安装hive


```bash
wget http://mirrors.hust.edu.cn/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz
tar zxvf apache-hive-1.2.1-bin.tar.gz
cd apache-hive-1.2.1-bin
cd conf
cp hive-default.xml.template   hive-site.xml
cp hive-exec-log4j.properties.template  hive-exec-log4j.properties
cp hive-log4j.properties.template  hive-log4j.properties
cp hive-env.sh.template hive-env.sh
vim hive-env.sh
HADOOP_HOME=/home/ebuy/hadoop/hadoop-2.6.4
export HIVE_CONF_DIR=/home/ebuy/hadoop/apache-hive-1.2.1-bin/conf
cat hive-site.xml 
<configuration>

<property>
    <!-- MySQ的URL配置 -->
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://144.232.134.58:3306/hive?createDatabaseIfNotExist=true</value>
</property>

<!-- 此处JDBC的驱动务必加上，对应的数据配置对应的驱动-->
<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
     <description>Driver class name for a JDBC metastore</description>
</property>

<!-- 数据库的用户名配置-->
<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
</property>

<!-- 数据库密码配置-->
<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive</value>
</property>

<!-- HDFS路径hive表的存放位置-->
<property>
    <name>hive.metastore.warehouse.dir</name>
    <value>hdfs://localhost:9000//user/hive/warehouse</value>
</property>

<!--HDFS路径，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。 -->
<property>
    <name>hive.exec.scratchdir</name>
    <value>/tmp</value>
</property>

<property>
    <name>mapred.child.java.opts</name>
    <value>-Xmx4096m</value>
</property>

<!-- 日志的记录位置-->
<property>
    <name>hive.querylog.location</name>
    <value>/home/ebuy/hadoop/apache-hive-1.2.1-bin/logs</value>
</property>

<property>
    <name>hive.metastore.local</name>
    <value>false</value>
</property>

<property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
</property>

<property>
    <name>hive.server2.thrift.bind.host</name>
    <value>localhost</value>
</property>

</configuration>

```

mysql连接驱动hive没有自带；需拷贝mysql-connector-java-5.1.21.jar到lib下,同时mysql据数据建表授权！

在HDFS上，新建hive的数据存储目录，以及MapReduce执行过程，生成的临时文件目录，执行命令如下，并赋值权限

```bash
hdfs dfs -mkidr /tmp  
hdfs dfs -mkidr /user/hive/warehouse  
hdfs dfs -chmod g+w /tmp  
hdfs dfs -chmod g+w /user/hive/warehouse  
```

## 启动hive 执行建表命令并导入数据
执行命令

``./bin/hive``

## Hiveserver2启动

HiveServer2作为一种服务接口，允许Hive远程客户端执行查询并返回结果；默认监听端口为10000。当前实现基于Thrift的RPC调用。是HiveServer的改进版本，支持多客户端的并发和权限控制.

``./bin/hiveserver2``

## fulme+HUE

[fulme+HUE配置][1]

```bash
vim hue/desktop/conf/pseudo-distributed.ini
hive_server_host=localhost
hive_server_port=10000
hive_conf_dir=/home/ebuy/hadoop/apache-hive-1.2.1-bin/conf
```

  [1]: http://linux48.com/2016-05-16-flume.html
